{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Ot9sAUYiRg"
      },
      "source": [
        "\n",
        "Optimization(Starting Point)\n",
        "==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWiR3jX7YiRf"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6pKRtumYiRh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ehkc0aYiRi"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf0O2pUyYiRj"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G_xGXjpYiRj"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSHw5BMoYiRk"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44fQrC1iYiRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250b7f2d-5e7c-4c13-d47f-da3ec260a049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.293545  [   64/60000]\n",
            "loss: 2.280405  [ 6464/60000]\n",
            "loss: 2.270022  [12864/60000]\n",
            "loss: 2.269761  [19264/60000]\n",
            "loss: 2.238204  [25664/60000]\n",
            "loss: 2.219436  [32064/60000]\n",
            "loss: 2.218393  [38464/60000]\n",
            "loss: 2.186449  [44864/60000]\n",
            "loss: 2.184035  [51264/60000]\n",
            "loss: 2.150966  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.7%, Avg loss: 2.145079 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.155477  [   64/60000]\n",
            "loss: 2.141022  [ 6464/60000]\n",
            "loss: 2.084610  [12864/60000]\n",
            "loss: 2.101702  [19264/60000]\n",
            "loss: 2.041961  [25664/60000]\n",
            "loss: 1.990088  [32064/60000]\n",
            "loss: 2.011439  [38464/60000]\n",
            "loss: 1.928666  [44864/60000]\n",
            "loss: 1.929951  [51264/60000]\n",
            "loss: 1.861056  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.1%, Avg loss: 1.858422 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.894958  [   64/60000]\n",
            "loss: 1.860572  [ 6464/60000]\n",
            "loss: 1.737391  [12864/60000]\n",
            "loss: 1.780388  [19264/60000]\n",
            "loss: 1.679196  [25664/60000]\n",
            "loss: 1.629976  [32064/60000]\n",
            "loss: 1.658584  [38464/60000]\n",
            "loss: 1.557987  [44864/60000]\n",
            "loss: 1.579853  [51264/60000]\n",
            "loss: 1.487198  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.8%, Avg loss: 1.501669 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.571409  [   64/60000]\n",
            "loss: 1.537166  [ 6464/60000]\n",
            "loss: 1.381805  [12864/60000]\n",
            "loss: 1.458011  [19264/60000]\n",
            "loss: 1.353509  [25664/60000]\n",
            "loss: 1.341705  [32064/60000]\n",
            "loss: 1.370909  [38464/60000]\n",
            "loss: 1.292048  [44864/60000]\n",
            "loss: 1.320382  [51264/60000]\n",
            "loss: 1.236930  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.0%, Avg loss: 1.256142 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.335713  [   64/60000]\n",
            "loss: 1.317769  [ 6464/60000]\n",
            "loss: 1.146180  [12864/60000]\n",
            "loss: 1.252661  [19264/60000]\n",
            "loss: 1.138095  [25664/60000]\n",
            "loss: 1.153808  [32064/60000]\n",
            "loss: 1.192645  [38464/60000]\n",
            "loss: 1.124699  [44864/60000]\n",
            "loss: 1.155299  [51264/60000]\n",
            "loss: 1.085704  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.100223 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.175229  [   64/60000]\n",
            "loss: 1.176631  [ 6464/60000]\n",
            "loss: 0.987637  [12864/60000]\n",
            "loss: 1.121316  [19264/60000]\n",
            "loss: 1.000052  [25664/60000]\n",
            "loss: 1.023385  [32064/60000]\n",
            "loss: 1.077478  [38464/60000]\n",
            "loss: 1.013571  [44864/60000]\n",
            "loss: 1.043325  [51264/60000]\n",
            "loss: 0.987298  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.2%, Avg loss: 0.995856 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.059382  [   64/60000]\n",
            "loss: 1.081977  [ 6464/60000]\n",
            "loss: 0.875519  [12864/60000]\n",
            "loss: 1.032001  [19264/60000]\n",
            "loss: 0.910494  [25664/60000]\n",
            "loss: 0.928157  [32064/60000]\n",
            "loss: 0.999555  [38464/60000]\n",
            "loss: 0.938114  [44864/60000]\n",
            "loss: 0.963300  [51264/60000]\n",
            "loss: 0.919539  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.8%, Avg loss: 0.922578 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.971575  [   64/60000]\n",
            "loss: 1.013999  [ 6464/60000]\n",
            "loss: 0.792958  [12864/60000]\n",
            "loss: 0.967855  [19264/60000]\n",
            "loss: 0.849848  [25664/60000]\n",
            "loss: 0.856448  [32064/60000]\n",
            "loss: 0.943530  [38464/60000]\n",
            "loss: 0.885702  [44864/60000]\n",
            "loss: 0.903941  [51264/60000]\n",
            "loss: 0.869592  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.868569 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.902706  [   64/60000]\n",
            "loss: 0.961756  [ 6464/60000]\n",
            "loss: 0.730047  [12864/60000]\n",
            "loss: 0.918916  [19264/60000]\n",
            "loss: 0.806383  [25664/60000]\n",
            "loss: 0.801217  [32064/60000]\n",
            "loss: 0.900829  [38464/60000]\n",
            "loss: 0.847965  [44864/60000]\n",
            "loss: 0.858840  [51264/60000]\n",
            "loss: 0.830933  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.6%, Avg loss: 0.827005 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.847073  [   64/60000]\n",
            "loss: 0.919272  [ 6464/60000]\n",
            "loss: 0.680509  [12864/60000]\n",
            "loss: 0.880426  [19264/60000]\n",
            "loss: 0.773746  [25664/60000]\n",
            "loss: 0.758259  [32064/60000]\n",
            "loss: 0.866324  [38464/60000]\n",
            "loss: 0.819573  [44864/60000]\n",
            "loss: 0.823698  [51264/60000]\n",
            "loss: 0.799755  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.793829 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yBf8hJGY4HM"
      },
      "source": [
        "\n",
        "Optimization(Add Dropout)\n",
        "==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPSWT_-5Y4HN"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtM9Yvj5Y4HN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGKlp2JSY4HN"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-Wteq9LY4HN"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gI1gIDXY4HN"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diwEF2dYY4HO"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH6BzRkuY4HO",
        "outputId": "909ab70e-3315-4bec-f096-23edc846d920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.293315  [   64/60000]\n",
            "loss: 2.297397  [ 6464/60000]\n",
            "loss: 2.281066  [12864/60000]\n",
            "loss: 2.276732  [19264/60000]\n",
            "loss: 2.264351  [25664/60000]\n",
            "loss: 2.256597  [32064/60000]\n",
            "loss: 2.249316  [38464/60000]\n",
            "loss: 2.222011  [44864/60000]\n",
            "loss: 2.211400  [51264/60000]\n",
            "loss: 2.219745  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 2.178388 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.218931  [   64/60000]\n",
            "loss: 2.188277  [ 6464/60000]\n",
            "loss: 2.154992  [12864/60000]\n",
            "loss: 2.166074  [19264/60000]\n",
            "loss: 2.122099  [25664/60000]\n",
            "loss: 2.063459  [32064/60000]\n",
            "loss: 2.103654  [38464/60000]\n",
            "loss: 2.068596  [44864/60000]\n",
            "loss: 2.039144  [51264/60000]\n",
            "loss: 2.019089  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.1%, Avg loss: 1.969304 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.032944  [   64/60000]\n",
            "loss: 1.982854  [ 6464/60000]\n",
            "loss: 1.950300  [12864/60000]\n",
            "loss: 1.954408  [19264/60000]\n",
            "loss: 1.878351  [25664/60000]\n",
            "loss: 1.838802  [32064/60000]\n",
            "loss: 1.837829  [38464/60000]\n",
            "loss: 1.775647  [44864/60000]\n",
            "loss: 1.763271  [51264/60000]\n",
            "loss: 1.692958  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.9%, Avg loss: 1.635702 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.759825  [   64/60000]\n",
            "loss: 1.728932  [ 6464/60000]\n",
            "loss: 1.582372  [12864/60000]\n",
            "loss: 1.624063  [19264/60000]\n",
            "loss: 1.593187  [25664/60000]\n",
            "loss: 1.485476  [32064/60000]\n",
            "loss: 1.528686  [38464/60000]\n",
            "loss: 1.460461  [44864/60000]\n",
            "loss: 1.480288  [51264/60000]\n",
            "loss: 1.387580  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 1.352836 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.481820  [   64/60000]\n",
            "loss: 1.432409  [ 6464/60000]\n",
            "loss: 1.358493  [12864/60000]\n",
            "loss: 1.437778  [19264/60000]\n",
            "loss: 1.324138  [25664/60000]\n",
            "loss: 1.312750  [32064/60000]\n",
            "loss: 1.341108  [38464/60000]\n",
            "loss: 1.269414  [44864/60000]\n",
            "loss: 1.321445  [51264/60000]\n",
            "loss: 1.160012  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.5%, Avg loss: 1.174432 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.292225  [   64/60000]\n",
            "loss: 1.315678  [ 6464/60000]\n",
            "loss: 1.213150  [12864/60000]\n",
            "loss: 1.250975  [19264/60000]\n",
            "loss: 1.175727  [25664/60000]\n",
            "loss: 1.133641  [32064/60000]\n",
            "loss: 1.239672  [38464/60000]\n",
            "loss: 1.197323  [44864/60000]\n",
            "loss: 1.263270  [51264/60000]\n",
            "loss: 1.164125  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.5%, Avg loss: 1.058551 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.267018  [   64/60000]\n",
            "loss: 1.277611  [ 6464/60000]\n",
            "loss: 1.120436  [12864/60000]\n",
            "loss: 1.200507  [19264/60000]\n",
            "loss: 1.116280  [25664/60000]\n",
            "loss: 1.110666  [32064/60000]\n",
            "loss: 1.098194  [38464/60000]\n",
            "loss: 1.094737  [44864/60000]\n",
            "loss: 1.061516  [51264/60000]\n",
            "loss: 1.094526  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.977741 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.102882  [   64/60000]\n",
            "loss: 1.234195  [ 6464/60000]\n",
            "loss: 0.983349  [12864/60000]\n",
            "loss: 1.087345  [19264/60000]\n",
            "loss: 1.022739  [25664/60000]\n",
            "loss: 1.051437  [32064/60000]\n",
            "loss: 1.137473  [38464/60000]\n",
            "loss: 1.100866  [44864/60000]\n",
            "loss: 1.015849  [51264/60000]\n",
            "loss: 1.057619  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.5%, Avg loss: 0.919500 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.148719  [   64/60000]\n",
            "loss: 1.053209  [ 6464/60000]\n",
            "loss: 0.836844  [12864/60000]\n",
            "loss: 1.056640  [19264/60000]\n",
            "loss: 0.931655  [25664/60000]\n",
            "loss: 1.014779  [32064/60000]\n",
            "loss: 1.007763  [38464/60000]\n",
            "loss: 1.017894  [44864/60000]\n",
            "loss: 0.965759  [51264/60000]\n",
            "loss: 1.006092  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 0.875308 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.981777  [   64/60000]\n",
            "loss: 1.031593  [ 6464/60000]\n",
            "loss: 0.876575  [12864/60000]\n",
            "loss: 1.065546  [19264/60000]\n",
            "loss: 0.898711  [25664/60000]\n",
            "loss: 0.937654  [32064/60000]\n",
            "loss: 1.062036  [38464/60000]\n",
            "loss: 0.888905  [44864/60000]\n",
            "loss: 0.985091  [51264/60000]\n",
            "loss: 0.992376  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.841501 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hrxCXELY9A4"
      },
      "source": [
        "\n",
        "Optimization(Add BatchNorm)\n",
        "==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX16jokJY9A4"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue2kj_iRY9A4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.BatchNorm1d(512),  # 배치 정규화 추가\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),  # 두 번째 배치 정규화 추가\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi5L3m6YY9A5"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LyuYuGdY9A5"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCIzm2jAY9A5"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXgTUUoFY9A5"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKATdFoiY9A5",
        "outputId": "691ccf2e-231d-416e-e1e9-05d038f32e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.403117  [   64/60000]\n",
            "loss: 1.500073  [ 6464/60000]\n",
            "loss: 0.986434  [12864/60000]\n",
            "loss: 1.106708  [19264/60000]\n",
            "loss: 0.933670  [25664/60000]\n",
            "loss: 0.818983  [32064/60000]\n",
            "loss: 0.803050  [38464/60000]\n",
            "loss: 0.771668  [44864/60000]\n",
            "loss: 0.791565  [51264/60000]\n",
            "loss: 0.750420  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.9%, Avg loss: 0.674230 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.606374  [   64/60000]\n",
            "loss: 0.691563  [ 6464/60000]\n",
            "loss: 0.455010  [12864/60000]\n",
            "loss: 0.757241  [19264/60000]\n",
            "loss: 0.695572  [25664/60000]\n",
            "loss: 0.588575  [32064/60000]\n",
            "loss: 0.606157  [38464/60000]\n",
            "loss: 0.650425  [44864/60000]\n",
            "loss: 0.673902  [51264/60000]\n",
            "loss: 0.597430  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.555802 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.480740  [   64/60000]\n",
            "loss: 0.560652  [ 6464/60000]\n",
            "loss: 0.363180  [12864/60000]\n",
            "loss: 0.652228  [19264/60000]\n",
            "loss: 0.596298  [25664/60000]\n",
            "loss: 0.511557  [32064/60000]\n",
            "loss: 0.518138  [38464/60000]\n",
            "loss: 0.607602  [44864/60000]\n",
            "loss: 0.622071  [51264/60000]\n",
            "loss: 0.518943  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.502154 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.421962  [   64/60000]\n",
            "loss: 0.500411  [ 6464/60000]\n",
            "loss: 0.318759  [12864/60000]\n",
            "loss: 0.589849  [19264/60000]\n",
            "loss: 0.532856  [25664/60000]\n",
            "loss: 0.471015  [32064/60000]\n",
            "loss: 0.463213  [38464/60000]\n",
            "loss: 0.577923  [44864/60000]\n",
            "loss: 0.583560  [51264/60000]\n",
            "loss: 0.471136  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.470514 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.382580  [   64/60000]\n",
            "loss: 0.466174  [ 6464/60000]\n",
            "loss: 0.290274  [12864/60000]\n",
            "loss: 0.546536  [19264/60000]\n",
            "loss: 0.487870  [25664/60000]\n",
            "loss: 0.444072  [32064/60000]\n",
            "loss: 0.425426  [38464/60000]\n",
            "loss: 0.551753  [44864/60000]\n",
            "loss: 0.551934  [51264/60000]\n",
            "loss: 0.441145  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.449050 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.354166  [   64/60000]\n",
            "loss: 0.442383  [ 6464/60000]\n",
            "loss: 0.270814  [12864/60000]\n",
            "loss: 0.514812  [19264/60000]\n",
            "loss: 0.453908  [25664/60000]\n",
            "loss: 0.422795  [32064/60000]\n",
            "loss: 0.396940  [38464/60000]\n",
            "loss: 0.528328  [44864/60000]\n",
            "loss: 0.526064  [51264/60000]\n",
            "loss: 0.419711  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.433011 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.330785  [   64/60000]\n",
            "loss: 0.425169  [ 6464/60000]\n",
            "loss: 0.255803  [12864/60000]\n",
            "loss: 0.489713  [19264/60000]\n",
            "loss: 0.426427  [25664/60000]\n",
            "loss: 0.404864  [32064/60000]\n",
            "loss: 0.374304  [38464/60000]\n",
            "loss: 0.507793  [44864/60000]\n",
            "loss: 0.500249  [51264/60000]\n",
            "loss: 0.402928  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.2%, Avg loss: 0.420461 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.310958  [   64/60000]\n",
            "loss: 0.411575  [ 6464/60000]\n",
            "loss: 0.243865  [12864/60000]\n",
            "loss: 0.470193  [19264/60000]\n",
            "loss: 0.404794  [25664/60000]\n",
            "loss: 0.390079  [32064/60000]\n",
            "loss: 0.356647  [38464/60000]\n",
            "loss: 0.489475  [44864/60000]\n",
            "loss: 0.477539  [51264/60000]\n",
            "loss: 0.388400  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 0.410349 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.295279  [   64/60000]\n",
            "loss: 0.399788  [ 6464/60000]\n",
            "loss: 0.234423  [12864/60000]\n",
            "loss: 0.453445  [19264/60000]\n",
            "loss: 0.386593  [25664/60000]\n",
            "loss: 0.377402  [32064/60000]\n",
            "loss: 0.343096  [38464/60000]\n",
            "loss: 0.472233  [44864/60000]\n",
            "loss: 0.456528  [51264/60000]\n",
            "loss: 0.376104  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.401958 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.281603  [   64/60000]\n",
            "loss: 0.390216  [ 6464/60000]\n",
            "loss: 0.226596  [12864/60000]\n",
            "loss: 0.438844  [19264/60000]\n",
            "loss: 0.371451  [25664/60000]\n",
            "loss: 0.365756  [32064/60000]\n",
            "loss: 0.331791  [38464/60000]\n",
            "loss: 0.456565  [44864/60000]\n",
            "loss: 0.438097  [51264/60000]\n",
            "loss: 0.365801  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.394773 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D-rAboKmheD"
      },
      "source": [
        "\n",
        "Optimization(Add Dropout and BatchNorm)\n",
        "==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B-eleKJmheK"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb3q3Y4HmheK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.BatchNorm1d(512),  # 배치 정규화 추가\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),  # 두 번째 배치 정규화 추가\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQx4v79tmheK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB0eIS-hmheK"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Egv8FlmheK"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT7QTvlMmheK"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d78fc1-664f-4572-f991-ace52fe092ba",
        "id": "12hffX-pmheL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300725  [   64/60000]\n",
            "loss: 1.882867  [ 6464/60000]\n",
            "loss: 1.562254  [12864/60000]\n",
            "loss: 1.453183  [19264/60000]\n",
            "loss: 1.342063  [25664/60000]\n",
            "loss: 1.190518  [32064/60000]\n",
            "loss: 1.202918  [38464/60000]\n",
            "loss: 1.086241  [44864/60000]\n",
            "loss: 1.076868  [51264/60000]\n",
            "loss: 1.036317  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.866210 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.952486  [   64/60000]\n",
            "loss: 1.017716  [ 6464/60000]\n",
            "loss: 0.763396  [12864/60000]\n",
            "loss: 0.985896  [19264/60000]\n",
            "loss: 0.905231  [25664/60000]\n",
            "loss: 0.906260  [32064/60000]\n",
            "loss: 0.864672  [38464/60000]\n",
            "loss: 0.864303  [44864/60000]\n",
            "loss: 0.914898  [51264/60000]\n",
            "loss: 0.829579  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.691621 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.760337  [   64/60000]\n",
            "loss: 0.767997  [ 6464/60000]\n",
            "loss: 0.628690  [12864/60000]\n",
            "loss: 0.835262  [19264/60000]\n",
            "loss: 0.836583  [25664/60000]\n",
            "loss: 0.725824  [32064/60000]\n",
            "loss: 0.828340  [38464/60000]\n",
            "loss: 0.727063  [44864/60000]\n",
            "loss: 0.827911  [51264/60000]\n",
            "loss: 0.779048  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.613467 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.608747  [   64/60000]\n",
            "loss: 0.692687  [ 6464/60000]\n",
            "loss: 0.494914  [12864/60000]\n",
            "loss: 0.874416  [19264/60000]\n",
            "loss: 0.778063  [25664/60000]\n",
            "loss: 0.630724  [32064/60000]\n",
            "loss: 0.756940  [38464/60000]\n",
            "loss: 0.672979  [44864/60000]\n",
            "loss: 0.771318  [51264/60000]\n",
            "loss: 0.710612  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.569077 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.571090  [   64/60000]\n",
            "loss: 0.676732  [ 6464/60000]\n",
            "loss: 0.441142  [12864/60000]\n",
            "loss: 0.866091  [19264/60000]\n",
            "loss: 0.603865  [25664/60000]\n",
            "loss: 0.619273  [32064/60000]\n",
            "loss: 0.641164  [38464/60000]\n",
            "loss: 0.710956  [44864/60000]\n",
            "loss: 0.787351  [51264/60000]\n",
            "loss: 0.654953  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.537997 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.607996  [   64/60000]\n",
            "loss: 0.645354  [ 6464/60000]\n",
            "loss: 0.474938  [12864/60000]\n",
            "loss: 0.743088  [19264/60000]\n",
            "loss: 0.706617  [25664/60000]\n",
            "loss: 0.579234  [32064/60000]\n",
            "loss: 0.579539  [38464/60000]\n",
            "loss: 0.638044  [44864/60000]\n",
            "loss: 0.828707  [51264/60000]\n",
            "loss: 0.650725  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.515391 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.581051  [   64/60000]\n",
            "loss: 0.549595  [ 6464/60000]\n",
            "loss: 0.418958  [12864/60000]\n",
            "loss: 0.681559  [19264/60000]\n",
            "loss: 0.644195  [25664/60000]\n",
            "loss: 0.529892  [32064/60000]\n",
            "loss: 0.606652  [38464/60000]\n",
            "loss: 0.662978  [44864/60000]\n",
            "loss: 0.606050  [51264/60000]\n",
            "loss: 0.564342  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.497719 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.522621  [   64/60000]\n",
            "loss: 0.495734  [ 6464/60000]\n",
            "loss: 0.408850  [12864/60000]\n",
            "loss: 0.629961  [19264/60000]\n",
            "loss: 0.658931  [25664/60000]\n",
            "loss: 0.613110  [32064/60000]\n",
            "loss: 0.589683  [38464/60000]\n",
            "loss: 0.639899  [44864/60000]\n",
            "loss: 0.660938  [51264/60000]\n",
            "loss: 0.531136  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.486613 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.494387  [   64/60000]\n",
            "loss: 0.573107  [ 6464/60000]\n",
            "loss: 0.392477  [12864/60000]\n",
            "loss: 0.677391  [19264/60000]\n",
            "loss: 0.687336  [25664/60000]\n",
            "loss: 0.544439  [32064/60000]\n",
            "loss: 0.517675  [38464/60000]\n",
            "loss: 0.664749  [44864/60000]\n",
            "loss: 0.711493  [51264/60000]\n",
            "loss: 0.596576  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.474458 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.460513  [   64/60000]\n",
            "loss: 0.531691  [ 6464/60000]\n",
            "loss: 0.387881  [12864/60000]\n",
            "loss: 0.710081  [19264/60000]\n",
            "loss: 0.530600  [25664/60000]\n",
            "loss: 0.545263  [32064/60000]\n",
            "loss: 0.521044  [38464/60000]\n",
            "loss: 0.659283  [44864/60000]\n",
            "loss: 0.769198  [51264/60000]\n",
            "loss: 0.557347  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.464792 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNBT_BwkZB0G"
      },
      "source": [
        "Optimization(Apply ADAM)\n",
        "==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVJ-lek6ZB0G"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWdkn7kPZB0G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j9CvQu_ZB0G"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzSVDjxCZB0H"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDHagAyjZB0H"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfn0CEzLZB0H"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LnaVdFJZB0H",
        "outputId": "514ae029-b46f-4e74-9b21-d2d172508ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298682  [   64/60000]\n",
            "loss: 2.283780  [ 6464/60000]\n",
            "loss: 2.260533  [12864/60000]\n",
            "loss: 2.256832  [19264/60000]\n",
            "loss: 2.229884  [25664/60000]\n",
            "loss: 2.198077  [32064/60000]\n",
            "loss: 2.214664  [38464/60000]\n",
            "loss: 2.167561  [44864/60000]\n",
            "loss: 2.172100  [51264/60000]\n",
            "loss: 2.140145  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.6%, Avg loss: 2.120937 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.137583  [   64/60000]\n",
            "loss: 2.125951  [ 6464/60000]\n",
            "loss: 2.054522  [12864/60000]\n",
            "loss: 2.071939  [19264/60000]\n",
            "loss: 2.011291  [25664/60000]\n",
            "loss: 1.952279  [32064/60000]\n",
            "loss: 1.982129  [38464/60000]\n",
            "loss: 1.890146  [44864/60000]\n",
            "loss: 1.901774  [51264/60000]\n",
            "loss: 1.827366  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.2%, Avg loss: 1.811634 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.860174  [   64/60000]\n",
            "loss: 1.824570  [ 6464/60000]\n",
            "loss: 1.688911  [12864/60000]\n",
            "loss: 1.729658  [19264/60000]\n",
            "loss: 1.616682  [25664/60000]\n",
            "loss: 1.583621  [32064/60000]\n",
            "loss: 1.597970  [38464/60000]\n",
            "loss: 1.501143  [44864/60000]\n",
            "loss: 1.534704  [51264/60000]\n",
            "loss: 1.430562  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.7%, Avg loss: 1.441052 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.524243  [   64/60000]\n",
            "loss: 1.489973  [ 6464/60000]\n",
            "loss: 1.327192  [12864/60000]\n",
            "loss: 1.402116  [19264/60000]\n",
            "loss: 1.287516  [25664/60000]\n",
            "loss: 1.295152  [32064/60000]\n",
            "loss: 1.302711  [38464/60000]\n",
            "loss: 1.232375  [44864/60000]\n",
            "loss: 1.275959  [51264/60000]\n",
            "loss: 1.181572  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.8%, Avg loss: 1.200534 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.289164  [   64/60000]\n",
            "loss: 1.274030  [ 6464/60000]\n",
            "loss: 1.095121  [12864/60000]\n",
            "loss: 1.205724  [19264/60000]\n",
            "loss: 1.086205  [25664/60000]\n",
            "loss: 1.115326  [32064/60000]\n",
            "loss: 1.131438  [38464/60000]\n",
            "loss: 1.071983  [44864/60000]\n",
            "loss: 1.119081  [51264/60000]\n",
            "loss: 1.041659  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.1%, Avg loss: 1.055859 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.134997  [   64/60000]\n",
            "loss: 1.141517  [ 6464/60000]\n",
            "loss: 0.945684  [12864/60000]\n",
            "loss: 1.084540  [19264/60000]\n",
            "loss: 0.964813  [25664/60000]\n",
            "loss: 0.994341  [32064/60000]\n",
            "loss: 1.026160  [38464/60000]\n",
            "loss: 0.971156  [44864/60000]\n",
            "loss: 1.016076  [51264/60000]\n",
            "loss: 0.954087  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.4%, Avg loss: 0.962375 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.026910  [   64/60000]\n",
            "loss: 1.054882  [ 6464/60000]\n",
            "loss: 0.843302  [12864/60000]\n",
            "loss: 1.002767  [19264/60000]\n",
            "loss: 0.887016  [25664/60000]\n",
            "loss: 0.907655  [32064/60000]\n",
            "loss: 0.955480  [38464/60000]\n",
            "loss: 0.905345  [44864/60000]\n",
            "loss: 0.943139  [51264/60000]\n",
            "loss: 0.893561  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 0.897310 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.946387  [   64/60000]\n",
            "loss: 0.993512  [ 6464/60000]\n",
            "loss: 0.769058  [12864/60000]\n",
            "loss: 0.943231  [19264/60000]\n",
            "loss: 0.833103  [25664/60000]\n",
            "loss: 0.842774  [32064/60000]\n",
            "loss: 0.904140  [38464/60000]\n",
            "loss: 0.860382  [44864/60000]\n",
            "loss: 0.889195  [51264/60000]\n",
            "loss: 0.848535  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 0.849206 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.883281  [   64/60000]\n",
            "loss: 0.946527  [ 6464/60000]\n",
            "loss: 0.712697  [12864/60000]\n",
            "loss: 0.897730  [19264/60000]\n",
            "loss: 0.793265  [25664/60000]\n",
            "loss: 0.792841  [32064/60000]\n",
            "loss: 0.864447  [38464/60000]\n",
            "loss: 0.828008  [44864/60000]\n",
            "loss: 0.848120  [51264/60000]\n",
            "loss: 0.813167  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 0.811889 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.832194  [   64/60000]\n",
            "loss: 0.908045  [ 6464/60000]\n",
            "loss: 0.668184  [12864/60000]\n",
            "loss: 0.861883  [19264/60000]\n",
            "loss: 0.762167  [25664/60000]\n",
            "loss: 0.753949  [32064/60000]\n",
            "loss: 0.832036  [38464/60000]\n",
            "loss: 0.803649  [44864/60000]\n",
            "loss: 0.815884  [51264/60000]\n",
            "loss: 0.784051  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.781722 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZWnfa29ZI5E"
      },
      "source": [
        "\n",
        "Optimization(RMSProp)\n",
        "==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3Uhbuc_ZI5L"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuDgjiCEZI5L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc2XasfQZI5M"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUUr_cEmZI5M"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXqZrue-ZI5M"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVEwq-amZI5M"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl-2eNmgZI5M",
        "outputId": "4248b395-f4f1-48fc-a7b7-84bf71f147d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.291366  [   64/60000]\n",
            "loss: 2.286340  [ 6464/60000]\n",
            "loss: 2.267192  [12864/60000]\n",
            "loss: 2.265940  [19264/60000]\n",
            "loss: 2.252255  [25664/60000]\n",
            "loss: 2.217203  [32064/60000]\n",
            "loss: 2.230924  [38464/60000]\n",
            "loss: 2.194880  [44864/60000]\n",
            "loss: 2.189865  [51264/60000]\n",
            "loss: 2.165128  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.5%, Avg loss: 2.156414 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.154964  [   64/60000]\n",
            "loss: 2.156057  [ 6464/60000]\n",
            "loss: 2.094995  [12864/60000]\n",
            "loss: 2.118923  [19264/60000]\n",
            "loss: 2.083694  [25664/60000]\n",
            "loss: 2.012886  [32064/60000]\n",
            "loss: 2.053005  [38464/60000]\n",
            "loss: 1.969585  [44864/60000]\n",
            "loss: 1.971161  [51264/60000]\n",
            "loss: 1.922202  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.5%, Avg loss: 1.906128 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.922845  [   64/60000]\n",
            "loss: 1.909132  [ 6464/60000]\n",
            "loss: 1.784359  [12864/60000]\n",
            "loss: 1.830472  [19264/60000]\n",
            "loss: 1.745453  [25664/60000]\n",
            "loss: 1.677821  [32064/60000]\n",
            "loss: 1.713313  [38464/60000]\n",
            "loss: 1.596670  [44864/60000]\n",
            "loss: 1.623557  [51264/60000]\n",
            "loss: 1.532442  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.532665 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.586718  [   64/60000]\n",
            "loss: 1.560945  [ 6464/60000]\n",
            "loss: 1.397800  [12864/60000]\n",
            "loss: 1.474221  [19264/60000]\n",
            "loss: 1.369284  [25664/60000]\n",
            "loss: 1.343916  [32064/60000]\n",
            "loss: 1.374613  [38464/60000]\n",
            "loss: 1.274885  [44864/60000]\n",
            "loss: 1.325464  [51264/60000]\n",
            "loss: 1.225365  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 1.246573 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.313049  [   64/60000]\n",
            "loss: 1.303054  [ 6464/60000]\n",
            "loss: 1.130586  [12864/60000]\n",
            "loss: 1.235562  [19264/60000]\n",
            "loss: 1.128622  [25664/60000]\n",
            "loss: 1.133581  [32064/60000]\n",
            "loss: 1.166877  [38464/60000]\n",
            "loss: 1.083559  [44864/60000]\n",
            "loss: 1.141514  [51264/60000]\n",
            "loss: 1.054052  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 1.075224 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.135017  [   64/60000]\n",
            "loss: 1.145758  [ 6464/60000]\n",
            "loss: 0.961297  [12864/60000]\n",
            "loss: 1.091449  [19264/60000]\n",
            "loss: 0.987569  [25664/60000]\n",
            "loss: 0.999777  [32064/60000]\n",
            "loss: 1.043954  [38464/60000]\n",
            "loss: 0.968370  [44864/60000]\n",
            "loss: 1.027471  [51264/60000]\n",
            "loss: 0.952596  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.1%, Avg loss: 0.969128 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.016135  [   64/60000]\n",
            "loss: 1.047332  [ 6464/60000]\n",
            "loss: 0.849709  [12864/60000]\n",
            "loss: 0.998668  [19264/60000]\n",
            "loss: 0.902638  [25664/60000]\n",
            "loss: 0.909181  [32064/60000]\n",
            "loss: 0.966408  [38464/60000]\n",
            "loss: 0.896619  [44864/60000]\n",
            "loss: 0.952232  [51264/60000]\n",
            "loss: 0.886902  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.2%, Avg loss: 0.899076 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.931819  [   64/60000]\n",
            "loss: 0.981138  [ 6464/60000]\n",
            "loss: 0.772151  [12864/60000]\n",
            "loss: 0.935131  [19264/60000]\n",
            "loss: 0.847519  [25664/60000]\n",
            "loss: 0.844525  [32064/60000]\n",
            "loss: 0.913250  [38464/60000]\n",
            "loss: 0.849834  [44864/60000]\n",
            "loss: 0.899777  [51264/60000]\n",
            "loss: 0.840732  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.849516 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.868093  [   64/60000]\n",
            "loss: 0.932379  [ 6464/60000]\n",
            "loss: 0.715183  [12864/60000]\n",
            "loss: 0.889130  [19264/60000]\n",
            "loss: 0.808279  [25664/60000]\n",
            "loss: 0.796374  [32064/60000]\n",
            "loss: 0.873766  [38464/60000]\n",
            "loss: 0.817676  [44864/60000]\n",
            "loss: 0.861064  [51264/60000]\n",
            "loss: 0.805789  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.812199 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.817196  [   64/60000]\n",
            "loss: 0.893752  [ 6464/60000]\n",
            "loss: 0.671151  [12864/60000]\n",
            "loss: 0.854233  [19264/60000]\n",
            "loss: 0.778232  [25664/60000]\n",
            "loss: 0.759378  [32064/60000]\n",
            "loss: 0.842183  [38464/60000]\n",
            "loss: 0.794115  [44864/60000]\n",
            "loss: 0.831037  [51264/60000]\n",
            "loss: 0.777955  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.782456 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}